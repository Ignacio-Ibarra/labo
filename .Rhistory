correr <- a[boolean]
b <- "envios"
correr[, b]
correr[, ..b]
lapply(eval.params, function(x){list(paste0("min.",x) = min(correr[, ..x]), paste0("max.",x) = max(correr[, ..x])})
lapply(eval.params, function(x){list(.paste0("min.",x) = min(correr[, ..x]), .paste0("max.",x) = max(correr[, ..x])})
x <- "envios"
paste0("min.",x)
list( paste0("min.",x))
lapply(eval.params, function(x){list(x = min(correr[, ..x]), x = max(correr[, ..x])})
lapply(eval.params, function(x){list(x = min(correr[, ..x]), x = max(correr[, ..x]))})
eval.params <- c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios")
lapply(eval.params, function(x){list(x = min(correr[, ..x]), x = max(correr[, ..x]))})
lapply(eval.params, function(x){list(x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
v <- lapply(eval.params, function(x){list(x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
v[1]
v <- unlist(lapply(eval.params, function(x){list(x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))}))
v
v <- lapply(eval.params, function(x){list(x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
v[[1]]
v[["num_iterations"]]
v["num_iterations"]
v[[["num_iterations"]]]
v
v <- lapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
v
v[1]
v[1]$variable
v[1]
v[1]
v[1][1]
v[[1]]
v[[1]]$variable
c(list(1), list(2))
c(list(1,2), list(2,3))
list(1,2)
c(1,2)
c(c(1,2),c(1,2))
lapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
as.data.frame(lapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))}))
apply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
lapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
mapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
apply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
correr %>% mutate(
minimo = across(.eval.params,min()),
maximo = accross(.eval.params, max())
correr %>% mutate(
correr %>% mutate(
minimo = across(.eval.params,min()),
maximo = across(.eval.params, max()))
correr %>% mutate(
minimo = across(cols = eval.params,min()),
maximo = across(cols = eval.params, max()))
correr %>%
select(eval.params) %>%
mutate(
minimo = across(everything(),min()),
maximo = across(everything(), max())
)
correr %>%
select(all_of(eval.params)) %>%
mutate(
minimo = across(everything(),min()),
maximo = across(everything(), max())
)
correr %>%
select(all_of(eval.params)) %>%
mutate(
minimo = across(everything(), min),
maximo = across(everything(), max)
)
correr %>%
select(all_of(eval.params)) %>%
summarise(
minimo = min()),
correr %>%
select(all_of(eval.params)) %>%
summarise(
minimo = min(),
maximo = max()
)
correr %>%
select(all_of(eval.params))
correr %>%
select(all_of(eval.params)) %>%
summarise(
minimo = across(everything(), min()),
maximo = across(everything(), max())
)
correr %>%
select(all_of(eval.params)) %>%
summarise_all(
minimo = min(),
maximo = max()
)
correr %>%
select(all_of(eval.params)) %>%
summarise_all(list(min, max)
correr %>%
correr %>%
select(all_of(eval.params)) %>%
summarise_all(list(min, max))
correr %>%
select(all_of(eval.params)) %>%
summarise(across(everything(), list(min = min, max = max))
correr %>%
correr %>%
select(all_of(eval.params)) %>%
summarise(across(everything(), list(min = min, max = max)))
correr %>%
select(all_of(eval.params)) %>%
summarise(across(everything(), list(MIN = min, MAX = max)))
lapply(eval.params, function(x){list("variable" = x, "min" = min(correr[, ..x]), "max" = max(correr[, ..x]))})
lapply(eval.params, function(x){list(x =  c(min(correr[, ..x]), max(correr[, ..x])))})
..x
lapply(eval.params, function(x){list(..x =  c(min(correr[, ..x]), max(correr[, ..x])))})
correr[, unlist(lapply(.SD, my.summary)), .SDcols = eval.params]
my.summary = function(x) list(min = min(x), max = max(x))
correr[, unlist(lapply(.SD, my.summary)), .SDcols = eval.params]
as.list(correr[, unlist(lapply(.SD, my.summary)), .SDcols = eval.params])
correr[, lapply(.SD, function(u){
sapply(funcs, function(f) do.call(f,list(u)))
})][, t(.SD)]
funcs = c('min', 'median')
correr[, lapply(.SD, function(u){
sapply(funcs, function(f) do.call(f,list(u)))
})][, t(.SD)]
correr[, lapply(.SD, function(u){
sapply(funcs, function(f) do.call(f,list(u)))
})][, .SDcols = eval.params]
correr[, lapply(.SD, function(u){
sapply(funcs, function(f) do.call(f,list(u)))
}), .SDcols = eval.params]
correr[, lapply(.SDcols = eval.params, function(u){sapply(funcs, function(f) do.call(f,list(u)))})]
correr[, lapply(.SD, function(u){sapply(funcs, function(f) do.call(f,list(u)))}), .SDcols = eval.params]
row.names(m) <- c("min", "max")
m  <- correr[, lapply(.SD, function(u){sapply(funcs, function(f) do.call(f,list(u)))}), .SDcols = eval.params]
row.names(m) <- c("min", "max")
m
View(m)
fine.params  <- correr[, lapply(.SD, function(u){sapply(funcs, function(f) do.call(f,list(u)))}), .SDcols = eval.params]
fine.params$learning_rate
fine.params$learning_rate[1]
View(fine.params)
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ..cols]
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ..cols]
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios")
correr <- eval.df[gan.bin=="level4", ..cols]
View(correr)
View(correr)
View(correr)
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
View(correr)
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
dia.mes <- format(Sys.Date(),"%d%m")
dir.create( "./exp/" )
kaggle.folder <- paste0("KA", dia.mes)
FIXED_PARAM <- list()
FIXED_PARAM$experimento  <- kaggle.folder
# FIXED_PARAM$input$dataset       <- "./exp/FE2809/FE2809_dataset.csv.gz" #contiene Feat.Eng
FIXED_PARAM$input$dataset       <- "./datasets/datasets_muestra25_comp2.csv" #muestra aleatoria en local de prueba
FIXED_PARAM$input$training      <- c( 202103 )
FIXED_PARAM$input$future        <- c( 202105 )
# PARAM$finalmodel$semilla           <- 635837
#Aqui empieza el programa
setwd( "~/buckets/b1" )
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
dia.mes <- format(Sys.Date(),"%d%m")
dir.create( "./exp/" )
kaggle.folder <- paste0("KA", dia.mes)
FIXED_PARAM <- list()
FIXED_PARAM$experimento  <- kaggle.folder
# FIXED_PARAM$input$dataset       <- "./exp/FE2809/FE2809_dataset.csv.gz" #contiene Feat.Eng
FIXED_PARAM$input$dataset       <- "./datasets/datasets_muestra25_comp2.csv" #muestra aleatoria en local de prueba
FIXED_PARAM$input$training      <- c( 202103 )
FIXED_PARAM$input$future        <- c( 202105 )
# PARAM$finalmodel$semilla           <- 635837
#cargo el dataset donde voy a entrenar
dataset  <- fread(FIXED_PARAM$input$dataset, stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
#set trabaja con la clase  POS = { BAJA+1, BAJA+2 }
#esta estrategia es MUY importante
dataset[ , clase01 := ifelse( clase_ternaria %in%  c("BAJA+2","BAJA+1"), 1L, 0L) ]
#--------------------------------------
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
# dir.create( "./exp/",  showWarnings = FALSE )
dir.create( paste0("./exp/", FIXED_PARAM$experimento, "/" ), showWarnings = FALSE )
setwd( paste0("./exp/", FIXED_PARAM$experimento, "/" ) )
#--------------------------------------
#establezco donde entreno
dataset[ , train  := 0L ]
dataset[ foto_mes %in% FIXED_PARAM$input$training, train  := 1L ]
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== PARAM$input$future ]
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
dia.mes <- format(Sys.Date(),"%d%m")
dir.create( "./exp/" )
kaggle.folder <- paste0("KA", dia.mes)
FIXED_PARAM <- list()
FIXED_PARAM$experimento  <- kaggle.folder
# FIXED_PARAM$input$dataset       <- "./exp/FE2809/FE2809_dataset.csv.gz" #contiene Feat.Eng
FIXED_PARAM$input$dataset       <- "./datasets/datasets_muestra25_comp2.csv" #muestra aleatoria en local de prueba
FIXED_PARAM$input$training      <- c( 202103 )
FIXED_PARAM$input$future        <- c( 202105 )
# PARAM$finalmodel$semilla           <- 635837
#cargo el dataset donde voy a entrenar
dataset  <- fread(FIXED_PARAM$input$dataset, stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
#set trabaja con la clase  POS = { BAJA+1, BAJA+2 }
#esta estrategia es MUY importante
dataset[ , clase01 := ifelse( clase_ternaria %in%  c("BAJA+2","BAJA+1"), 1L, 0L) ]
#--------------------------------------
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
# dir.create( "./exp/",  showWarnings = FALSE )
dir.create( paste0("./exp/", FIXED_PARAM$experimento, "/" ), showWarnings = FALSE )
setwd( paste0("./exp/", FIXED_PARAM$experimento, "/" ) )
#--------------------------------------
#establezco donde entreno
dataset[ , train  := 0L ]
dataset[ foto_mes %in% FIXED_PARAM$input$training, train  := 1L ]
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            param.list$max_bin,
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
#establezco donde entreno
dataset[ , train  := 0L ]
dataset[ foto_mes %in% FIXED_PARAM$input$training, train  := 1L ]
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837,
feature_pre_filter= FALSE
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
