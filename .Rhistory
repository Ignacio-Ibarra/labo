require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
dia.mes <- format(Sys.Date(),"%d%m")
dir.create( "./exp/" )
kaggle.folder <- paste0("KA", dia.mes)
FIXED_PARAM <- list()
FIXED_PARAM$experimento  <- kaggle.folder
# FIXED_PARAM$input$dataset       <- "./exp/FE2809/FE2809_dataset.csv.gz" #contiene Feat.Eng
FIXED_PARAM$input$dataset       <- "./datasets/datasets_muestra25_comp2.csv" #muestra aleatoria en local de prueba
FIXED_PARAM$input$training      <- c( 202103 )
FIXED_PARAM$input$future        <- c( 202105 )
# PARAM$finalmodel$semilla           <- 635837
#cargo el dataset donde voy a entrenar
dataset  <- fread(FIXED_PARAM$input$dataset, stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
#set trabaja con la clase  POS = { BAJA+1, BAJA+2 }
#esta estrategia es MUY importante
dataset[ , clase01 := ifelse( clase_ternaria %in%  c("BAJA+2","BAJA+1"), 1L, 0L) ]
#--------------------------------------
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
# dir.create( "./exp/",  showWarnings = FALSE )
dir.create( paste0("./exp/", FIXED_PARAM$experimento, "/" ), showWarnings = FALSE )
setwd( paste0("./exp/", FIXED_PARAM$experimento, "/" ) )
#--------------------------------------
#establezco donde entreno
dataset[ , train  := 0L ]
dataset[ foto_mes %in% FIXED_PARAM$input$training, train  := 1L ]
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            param.list$max_bin,
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
#establezco donde entreno
dataset[ , train  := 0L ]
dataset[ foto_mes %in% FIXED_PARAM$input$training, train  := 1L ]
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ train==1L, campos_buenos, with=FALSE]),
label= dataset[ train==1L, clase01] )
#aplico el modelo a los datos sin clase
dapply  <- dataset[ foto_mes== FIXED_PARAM$input$future ]
fit.predict = function(param.list, train.set, test.set){
iteracion <- param.list$iteracion
#genero el modelo
#estos hiperparametros  salieron de una laaarga Optmizacion Bayesiana
modelo  <- lgb.train( data= train.set,
param= list( objective=          "binary",
max_bin=            31, #lo mantengo fijo
learning_rate=      param.list$learning_rate,
num_iterations=     param.list$num_iterations,
num_leaves=         param.list$num_leaves,
min_data_in_leaf=   param.list$min_data_in_leaf,
feature_fraction=   param.list$feature_fraction,
seed= 635837,
feature_pre_filter= FALSE
)
)
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( test.set[, campos_buenos, with=FALSE ])                                 )
#genero la tabla de entrega
tb_entrega  <-  test.set[ , list( numero_de_cliente, foto_mes ) ]
tb_entrega[  , prob := prediccion ]
# #grabo las probabilidad del modelo
# fwrite( tb_entrega,
#         file= "prediccion.txt",
#         sep= "\t" )
#ordeno por probabilidad descendente
setorder( tb_entrega, -prob )
#genero archivos con los  "envios" mejores
#deben subirse "inteligentemente" a Kaggle para no malgastar submits
envios <- param.list$envios
tb_entrega[  , Predicted := 0L ]
tb_entrega[ 1:envios, Predicted := 1L ]
fwrite( tb_entrega[ , list(numero_de_cliente, Predicted)],
file= paste0( FIXED_PARAM$experimento ,iteracion, ".csv" ),
sep= "," )
}
lapply(1:length(correr), function(i) fit.predict(correr[i], dtrain, dapply))
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
# para correr el Google Cloud
#   8 vCPU
#  64 GB memoria RAM
# 256 GB espacio en disco
# son varios archivos, subirlos INTELIGENTEMENTE a Kaggle
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("lightgbm")
require("gtools")
#setwd("~/buckets/b1/")
setwd("~/Desktop/EyF 2022")
bayesian.opt.output <- "./exp/HT7231/HT7231.txt"
experimentos <- fread(bayesian.opt.output)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
cols <-  c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios", "iteracion")
correr <- eval.df[gan.bin=="level4", ..cols]
View(correr)
View(eval.df)
View(experimentos)
a <- experimentos[ganancia>= 2.2e+07, ]
a <- experimentos[ganancia>= 2.3e+07, ]
a <- experimentos[ganancia>= 2.5e+07, ]
a <- experimentos[ganancia>= 2.55e+07, ]
a <- experimentos[ganancia>= 2.6e+07, ]
a <- experimentos[ganancia>= 2.63e+07, ]
a <- experimentos[ganancia>= 2.645e+07, ]
a <- experimentos[ganancia>= 2.649e+07, ]
a <- experimentos[ganancia>= 2.649e+07, ]
View(a)
View(correr)
eval.df <- experimentos[ganancia>= 2.65e+07, ]
bins <- 4
eval.df$gan.bin <- quantcut(eval.df$ganancia, q = bins, labels = paste0(c("level"),1:bins))
View(eval.df)
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require(data.table)
require(ggplot2)
require(gtools)
require(ggfortify)
require(gridExtra)
require(dplyr)
require(grid)
require(kableExtra)
# require(akmedoids)
setwd("~/Desktop/EyF 2022")
exp7231 <- fread("./exp/HT7231/HT7231.txt")
ggplot(exp7231, aes(x=feature_fraction, y=learning_rate))+geom_point()
ggplot(exp7231, aes(x=mean_data_leaf, y=num_leaves))+geom_point()
ggplot(exp7231, aes(x=min_data_leaf, y=num_leaves))+geom_point()
ggplot(exp7231, aes(x=min_data_in_leaf, y=num_leaves))+geom_point()
subs <- c("num_iterations","learning_rate","feature_fraction","min_data_in_leaf","num_leaves","envios","ganancia")
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(#"num_iterations",
"learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
# "envios",
"ganancia")
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(exp7231, aes(x=learning_rate, y=ganancia))+geom_point()
subs <- c(#"num_iterations",
"learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
"envios",
"ganancia")
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(#"num_iterations",
# "learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
"envios",
"ganancia")
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(exp7231, aes(x=envios, y=ganancia))+geom_point()
subs <- c(#"num_iterations",
# "learning_rate",
"feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(exp7231, aes(x=feature_fraction, y=ganancia))+geom_point()
subs <- c(#"num_iterations",
# "learning_rate",
"feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(reg, aes(x=feature_fraction, y=ganancia))+geom_point()
subs <- c(#"num_iterations",
"learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(reg, aes(x=learning_rate, y=ganancia))+geom_point()
subs <- c( "num_iterations",
# "learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(# "num_iterations",
# "learning_rate",
# "feature_fraction",
"min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(reg, aes(x=min_data_in_leaf, y=ganancia))+geom_point()
ggplot(reg, aes(x=min_data_in_leaf, y=num_leaves))+geom_point()
subs <- c(# "num_iterations",
# "learning_rate",
# "feature_fraction",
"min_data_in_leaf",
"num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(# "num_iterations",
# "learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
"num_leaves",
#"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(# "num_iterations",
# "learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
"envios",
"ganancia")
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
"num_iterations",
"learning_rate",
"feature_fraction",
"min_data_in_leaf",
"num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
# "num_iterations",
# "learning_rate",
# "feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
# "num_iterations",
# "learning_rate",
"feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia"
)
reg <- exp7231[ganancia>= 2.65e+07 , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
# "num_iterations",
# "learning_rate",
"feature_fraction",
# "min_data_in_leaf",
# "num_leaves",
#"envios",
"ganancia"
)
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
"num_iterations",
"learning_rate",
"feature_fraction",
"min_data_in_leaf",
"num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
ggplot(reg, aes(x=learning_rate, y=ganancia))+geom_point()
subs <- c(
#          "num_iterations",
"learning_rate",
"feature_fraction",
#          "min_data_in_leaf",
#          "num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
subs <- c(
#          "num_iterations",
"learning_rate",
"feature_fraction",
#          "min_data_in_leaf",
"num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ ., data = reg)
summary(mod1)
mod1 <- lm(ganancia ~ num_leaves**2, data = reg)
summary(mod1)
mod1 <- lm(ganancia ~ num_leaves**2 + min_data_in_leaf**2, data = reg)
summary(mod1)
mod1 <- lm(ganancia ~ num_leaves**2 + min_data_in_leaf**2, data = reg)
subs <- c(
"num_iterations",
"learning_rate",
"feature_fraction",
"min_data_in_leaf",
"num_leaves",
"envios",
"ganancia"
)
reg <- exp7231[ , ..subs]
mod1 <- lm(ganancia ~ num_leaves**2 + min_data_in_leaf**2, data = reg)
summary(mod1)
mod1 <- lm(ganancia ~ num_leaves * min_data_in_leaf, data = reg)
summary(mod1)
cor(reg)
cor(reg %>% select(!ganancia))
load("C:/Users/ignac/OneDrive/Escritorio/EyF 2022/exp/HT9420/bayesiana.RDATA")
bayesiana
View(opt.state)
data(bayesiana)
setwd("C:/Users/ignac/OneDrive/Escritorio/EyF 2022/exp/HT9420")
require(data.table)
setwd("C:/Users/ignac/OneDrive/Escritorio/EyF 2022/exp/HT9420")
bayopt <- fread("BO_log.txt")
View(bayopt)
